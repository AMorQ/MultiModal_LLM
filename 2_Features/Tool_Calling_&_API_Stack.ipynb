{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool Calling Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "height": 64
   },
   "source": [
    "Tool/Function calling in the LLama 3.1 and 3.2 models. \n",
    "\n",
    "It allows to more sophisticated tasks like real-time info, performing complex maths, coding assistance or building agentic applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from utils import get_tavily_api_key #for search\n",
    "TAVILY_API_KEY = get_tavily_api_key() # personalize this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "height": 30
   },
   "source": [
    "## how it works? \n",
    "The user informs Llama in the Prompt of the available tools to support the task the user is asking for. \n",
    "LLama responds with a direct answer or an indication tool and optimal parameters that should be used by the user and then send it back to Llama. \n",
    "\n",
    "It can also be handled automatically by the API/Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from utils import llama31\n",
    "from utils import cprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define tool system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09 March 2025\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_date = datetime.now()\n",
    "formatted_date = current_date.strftime(\"%d %B %Y\")\n",
    "print(formatted_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "tool_system_prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: {formatted_date}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The brave_search built-in tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "prompt = tool_system_prompt + f\"\"\"<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "What is the current weather in Menlo Park, California?\n",
    "\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "brave_search.call(query=\"current weather in Menlo Park, California\")\n"
     ]
    }
   ],
   "source": [
    "response = llama31(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "no_tool_call_prompt = tool_system_prompt + f\"\"\"<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "What is the population of California?\n",
    "\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "As of my knowledge cutoff in 2023, the estimated population of California is approximately 39.5 million people. However, please note that this information may have changed since my knowledge cutoff date. For the most up-to-date information, I recommend checking with the United States Census Bureau or other reliable sources.\n"
     ]
    }
   ],
   "source": [
    "no_tool_call_response = llama31(no_tool_call_prompt)\n",
    "print(no_tool_call_response)\n",
    "#the model decided this is sth that doesn't need a tool for (not real-time data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the search API"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "height": 115,
    "ExecuteTime": {
     "end_time": "2025-04-13T16:11:45.425889Z",
     "start_time": "2025-04-13T16:11:44.078063Z"
    }
   },
   "source": [
    "from tavily import TavilyClient\n",
    "tavily_client = TavilyClient(api_key=TAVILY_API_KEY)\n",
    "\n",
    "result = tavily_client.search(\"current weather in Menlo Park, California\")\n",
    "cprint(result)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TAVILY_API_KEY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtavily\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m TavilyClient\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m tavily_client = TavilyClient(api_key=\u001B[43mTAVILY_API_KEY\u001B[49m)\n\u001B[32m      4\u001B[39m result = tavily_client.search(\u001B[33m\"\u001B[39m\u001B[33mcurrent weather in Menlo Park, California\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      5\u001B[39m cprint(result)\n",
      "\u001B[31mNameError\u001B[39m: name 'TAVILY_API_KEY' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reprompting Llama with search tool response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'location': {'name': 'Menlo Park', 'region': 'California', 'country': 'United States of America', 'lat': 37.4539, 'lon': -122.1811, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1741527030, 'localtime': '2025-03-09 06:30'}, 'current': {'last_updated_epoch': 1741527000, 'last_updated': '2025-03-09 06:30', 'temp_c': 6.1, 'temp_f': 43.0, 'is_day': 0, 'condition': {'text': 'Clear', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 3.4, 'wind_kph': 5.4, 'wind_degree': 147, 'wind_dir': 'SSE', 'pressure_mb': 1022.0, 'pressure_in': 30.18, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 75, 'cloud': 0, 'feelslike_c': 5.2, 'feelslike_f': 41.3, 'windchill_c': 7.0, 'windchill_f': 44.6, 'heatindex_c': 7.3, 'heatindex_f': 45.1, 'dewpoint_c': 6.6, 'dewpoint_f': 43.9, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 0.0, 'gust_mph': 7.0, 'gust_kph': 11.3}}\n"
     ]
    }
   ],
   "source": [
    "search_result = result[\"results\"][0][\"content\"]\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "height": 251
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Environment: ipython\n",
      "Tools: brave_search, wolfram_alpha\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 09 March 2025\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "What is the current weather in Menlo Park, California?\n",
      "\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "<|python_tag|>\n",
      "\n",
      "brave_search.call(query=\"current weather in Menlo Park, California\")<|eom_id|>\n",
      "<|start_header_id|>ipython<|end_header_id|>\n",
      "\n",
      "{'location': {'name': 'Menlo Park', 'region': 'California', 'country': 'United States of America', 'lat': 37.4539, 'lon': -122.1811, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1741527030, 'localtime': '2025-03-09 06:30'}, 'current': {'last_updated_epoch': 1741527000, 'last_updated': '2025-03-09 06:30', 'temp_c': 6.1, 'temp_f': 43.0, 'is_day': 0, 'condition': {'text': 'Clear', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 3.4, 'wind_kph': 5.4, 'wind_degree': 147, 'wind_dir': 'SSE', 'pressure_mb': 1022.0, 'pressure_in': 30.18, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 75, 'cloud': 0, 'feelslike_c': 5.2, 'feelslike_f': 41.3, 'windchill_c': 7.0, 'windchill_f': 44.6, 'heatindex_c': 7.3, 'heatindex_f': 45.1, 'dewpoint_c': 6.6, 'dewpoint_f': 43.9, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 0.0, 'gust_mph': 7.0, 'gust_kph': 11.3}}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = tool_system_prompt + f\"\"\"<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "What is the current weather in Menlo Park, California?\n",
    "\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "<|python_tag|>{response}<|eom_id|> \n",
    "<|start_header_id|>ipython<|end_header_id|>\n",
    "\n",
    "{search_result}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "print(prompt)\n",
    "\n",
    "#response is the application provided (search) for the user question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current weather in Menlo Park, California is clear with a temperature of 6.1°C (43.0°F) and a wind speed of 5.4 km/h (3.4 mph) from the southeast. The humidity is 75% and the pressure is 1022 mb (30.18 in). There is no precipitation and the visibility is 16 km (9.9 miles).\n"
     ]
    }
   ],
   "source": [
    "response = llama31(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the higher-level message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "system_prompt_content = f\"\"\"\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: {formatted_date}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\":  system_prompt_content},\n",
    "    {\"role\": \"user\",   \"content\": \"What is the current weather in Menlo Park, California?\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brave_search.call(query=\"current weather in Menlo Park, California\")\n"
     ]
    }
   ],
   "source": [
    "response = llama31(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\",     \"content\":  system_prompt_content},\n",
    "    {\"role\": \"user\",       \"content\": \"What is the current weather in Menlo Park, California?\"},\n",
    "    {\"role\": \"assistant\",  \"content\": response},\n",
    "    {\"role\": \"ipython\",    \"content\": search_result}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current weather in Menlo Park, California is clear with a temperature of 6.1°C (43.0°F) and a wind speed of 3.4 mph (5.4 km/h) from the southeast. The humidity is 75% and the pressure is 1022 mb (30.18 in). There is no precipitation and the visibility is 16 km (9.9 miles). The UV index is 0.0 and the wind gust is 7.0 mph (11.3 km/h).\n"
     ]
    }
   ],
   "source": [
    "response = llama31(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Wolfram Alpha tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "math_problem = \"Can you help me solve this equation: x^3 - 2x^2 - x + 2 = 0?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\",  \"content\": system_prompt_content},\n",
    "    {\"role\": \"user\",    \"content\": math_problem}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sympy as sp\n",
      "\n",
      "# Define the variable\n",
      "x = sp.symbols('x')\n",
      "\n",
      "# Define the equation\n",
      "equation = x**3 - 2*x**2 - x + 2\n",
      "\n",
      "# Solve the equation\n",
      "solutions = sp.solve(equation, x)\n",
      "\n",
      "print(solutions)\n"
     ]
    }
   ],
   "source": [
    "response = llama31(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the Wolfram Alpha tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = ± 1\n",
      "x = 2\n"
     ]
    }
   ],
   "source": [
    "from utils import wolfram_alpha\n",
    "tool_result = wolfram_alpha(\"solve x^3 - 2x^2 - x + 2 = 0\")\n",
    "print(tool_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "height": 149
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "from sympy import symbols, Eq, solve\n",
    "\n",
    "x = symbols('x')                           # Define the variable\n",
    "equation = Eq(x**3 - 2*x**2 - 1*x + 2, 0) # Define the equation\n",
    "solution = solve(equation, x)              # Solve the equation\n",
    "\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reprompting Llama with Wolfram Alpha tool response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt_content},\n",
    "    {\"role\": \"user\",      \"content\": math_problem},\n",
    "    {\"role\": \"assistant\", \"content\": response},\n",
    "    {\"role\": \"ipython\",   \"content\": tool_result}\n",
    "]\n",
    "#in this case, the response is not the app, but the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The solutions to the equation x^3 - 2x^2 - x + 2 = 0 are x = 1, x = -1, and x = 2.\n"
     ]
    }
   ],
   "source": [
    "response = llama31(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code interpreter built-in tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "loan_question = (\n",
    "    \"How much is the monthly payment, total payment, \"\n",
    "    \"and total interest paid for a 30 year mortgage of $1M \"\n",
    "    \"at a fixed rate of 6% with a 20% down payment?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\",     \"content\": system_prompt_content},\n",
    "    {\"role\": \"user\",       \"content\": loan_question},\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import math\n",
      "\n",
      "# Define the variables\n",
      "loan_amount = 1000000  # $1M\n",
      "interest_rate = 0.06  # 6%\n",
      "loan_term = 30  # 30 years\n",
      "down_payment = 200000  # 20% of $1M\n",
      "\n",
      "# Calculate the loan amount after down payment\n",
      "loan_amount_after_down_payment = loan_amount - down_payment\n",
      "\n",
      "# Calculate the monthly interest rate\n",
      "monthly_interest_rate = interest_rate / 12\n",
      "\n",
      "# Calculate the number of payments\n",
      "number_of_payments = loan_term * 12\n",
      "\n",
      "# Calculate the monthly payment\n",
      "monthly_payment = loan_amount_after_down_payment * (monthly_interest_rate * (1 + monthly_interest_rate) ** number_of_payments) / ((1 + monthly_interest_rate) ** number_of_payments - 1)\n",
      "\n",
      "# Calculate the total payment\n",
      "total_payment = monthly_payment * number_of_payments\n",
      "\n",
      "# Calculate the total interest paid\n",
      "total_interest_paid = total_payment - loan_amount_after_down_payment\n",
      "\n",
      "print(f\"Monthly payment: ${math.ceil(monthly_payment)}\")\n",
      "print(f\"Total payment: ${math.ceil(total_payment)}\")\n",
      "print(f\"Total interest paid: ${math.ceil(total_interest_paid)}\")\n"
     ]
    }
   ],
   "source": [
    "response = llama31(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Use Case: Calculate loan payments"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "height": 200
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monthly payment: $4796\n",
      "Total payment: $1726705\n",
      "Total interest paid: $926705\n"
     ]
    }
   ],
   "source": [
    "from utils import calculate_loan\n",
    "monthly_payment, total_payment, total_interest_paid = calculate_loan(\n",
    "    loan_amount = 1000000, \n",
    "    annual_interest_rate = 0.06, \n",
    "    loan_term = 30, \n",
    "    down_payment = 200000)\n",
    "\n",
    "print(f\"Monthly payment: ${(monthly_payment)}\")\n",
    "print(f\"Total payment: ${(total_payment)}\")\n",
    "print(f\"Total interest paid: ${(total_interest_paid)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the code in Java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\":    \"system\",     \n",
    "     \"content\": system_prompt_content + \"\\nGenerate the code in Java.\"},\n",
    "    {\"role\":    \"user\",       \"content\": loan_question},\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import java.util.Scanner;\n",
      "\n",
      "public class MortgageCalculator {\n",
      "    public static void main(String[] args) {\n",
      "        Scanner scanner = new Scanner(System.in);\n",
      "\n",
      "        // Input values\n",
      "        double principal = 1000000; // $1M\n",
      "        double downPayment = 0.2 * principal; // 20% of $1M\n",
      "        double loanAmount = principal - downPayment; // $800,000\n",
      "        double annualInterestRate = 0.06; // 6%\n",
      "        int years = 30;\n",
      "\n",
      "        // Calculate monthly interest rate\n",
      "        double monthlyInterestRate = annualInterestRate / 12;\n",
      "\n",
      "        // Calculate number of payments\n",
      "        int numberOfPayments = years * 12;\n",
      "\n",
      "        // Calculate monthly payment\n",
      "        double monthlyPayment = loanAmount * monthlyInterestRate * Math.pow(1 + monthlyInterestRate, numberOfPayments) / (Math.pow(1 + monthlyInterestRate, numberOfPayments) - 1);\n",
      "\n",
      "        // Calculate total payment\n",
      "        double totalPayment = monthlyPayment * numberOfPayments;\n",
      "\n",
      "        // Calculate total interest paid\n",
      "        double totalInterestPaid = totalPayment - loanAmount;\n",
      "\n",
      "        // Print results\n",
      "        System.out.println(\"Monthly Payment: $\" + String.format(\"%.2f\", monthlyPayment));\n",
      "        System.out.println(\"Total Payment: $\" + String.format(\"%.2f\", totalPayment));\n",
      "        System.out.println(\"Total Interest Paid: $\" + String.format(\"%.2f\", totalInterestPaid));\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = llama31(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reprompting Llama with Code Interpreter tool response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "code_interpreter_tool_response=\"\"\"\n",
    "Monthly payment: $4796\n",
    "Total payment: $1726705\n",
    "Total interest paid: $926705\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\":  system_prompt_content},\n",
    "    {\"role\": \"user\",      \"content\": loan_question},\n",
    "    {\"role\": \"assistant\", \"content\": response},\n",
    "    {\"role\": \"ipython\",   \"content\": code_interpreter_tool_response}\n",
    "  ]\n",
    "#I provide the response given in a code and some data and it interprets that I want it to perform the operations in the code over that data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The monthly payment for a 30-year mortgage of $1M at a fixed rate of 6% with a 20% down payment is $4,796. The total payment over the life of the loan is $1,726,705. The total interest paid over the life of the loan is $926,705.\n"
     ]
    }
   ],
   "source": [
    "response = llama31(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1 custom tool calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "height": 132
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 trending songs in US:\n",
      "['Blinding Lights - The Weeknd', 'Levitating - Dua Lipa', 'Peaches - Justin Bieber', 'Save Your Tears - The Weeknd', 'Good 4 U - Olivia Rodrigo']\n"
     ]
    }
   ],
   "source": [
    "from utils import trending_songs, get_boiling_point\n",
    "\n",
    "country = \"US\"\n",
    "top_num = 5\n",
    "top_songs = trending_songs(country, top_num)\n",
    "print(f\"Top {top_num} trending songs in {country}:\")\n",
    "print(top_songs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Llama 3.1 for custom tool call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "height": 557
   },
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"\n",
    "Answer the user's question by using the following functions if needed.\n",
    "If none of the functions can be used, please say so.\n",
    "Functions (in JSON format):\n",
    "{\n",
    "    \"type\": \"function\", \"function\": {\n",
    "        \"name\": \"get_boiling_point\",\n",
    "        \"description\": \"Get the boiling point of a liquid\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\", \"properties\": [\n",
    "                {\"liquid_name\": {\"type\": \"object\", \"description\": \"name of the liquid\"}},\n",
    "                {\"celsius\": {\"type\": \"object\", \"description\": \"whether to use celsius\"}}\n",
    "            ], \"required\": [\"liquid_name\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "{\n",
    "    \"type\": \"function\", \"function\": {\n",
    "        \"name\": \"trending_songs\",\n",
    "        \"description\": \"Returns the trending songs on a Music site\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\", \"properties\": [\n",
    "                {\"country\": {\"type\": \"object\", \"description\": \"country to return trending songs for\"}},\n",
    "                {\"n\": {\"type\": \"object\", \"description\": \"The number of songs to return\"}}\n",
    "            ], \"required\": [\"country\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "Question: Can you check the top 5 trending songs in US?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "      \"role\": \"system\", \"content\":  f\"\"\"\n",
    "Environment: ipython\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: {formatted_date}\n",
    "\"\"\"},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"function\", \"name\": \"trending_songs\", \"parameters\": {\"country\": \"US\", \"n\": \"5\"}}\n"
     ]
    }
   ],
   "source": [
    "result = llama31(messages,405)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the custom tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "custom_tools = {\"trending_songs\": trending_songs,\n",
    "                \"get_boiling_point\": get_boiling_point}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('trending_songs', ['US', '5'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = json.loads(result)\n",
    "function_name = res['name']\n",
    "parameters = list(res['parameters'].values())\n",
    "function_name, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Blinding Lights - The Weeknd',\n",
       " 'Levitating - Dua Lipa',\n",
       " 'Peaches - Justin Bieber',\n",
       " 'Save Your Tears - The Weeknd',\n",
       " 'Good 4 U - Olivia Rodrigo']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_result = custom_tools[function_name](*parameters)\n",
    "#* = allow an arbitrary number of inputs\n",
    "tool_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reprompting Llama with custom tool call result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "height": 234
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "      \"role\": \"system\", \"content\":  f\"\"\"\n",
    "Environment: ipython\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: {formatted_date}\n",
    "\"\"\"},\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "    {\"role\": \"assistant\", \"content\": result},\n",
    "    {\"role\": \"ipython\", \"content\": ','.join(tool_result)}\n",
    "  ]\n",
    "#its needed to use ipython when making use of a custom function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 trending songs in the US are:\n",
      "\n",
      "1. Blinding Lights - The Weeknd\n",
      "2. Levitating - Dua Lipa\n",
      "3. Peaches - Justin Bieber\n",
      "4. Save Your Tears - The Weeknd\n",
      "5. Good 4 U - Olivia Rodrigo\n"
     ]
    }
   ],
   "source": [
    "response = llama31(messages, 70)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "height": 336
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some guidelines to start a statistical analysis:\n",
      "\n",
      "**I. Understanding the Data**\n",
      "\n",
      "* **Explore the data**: Look at the distribution of the variables, check for missing values, and identify any outliers.\n",
      "* **Understand the type of variables**: Identify the type of variables you are working with (e.g., continuous, categorical, ordinal).\n",
      "* **Check for normality**: Use plots (e.g., Q-Q plot, histogram) and statistical tests (e.g., Shapiro-Wilk test) to check if the data follows a normal distribution.\n",
      "\n",
      "**II. Descriptive Statistics**\n",
      "\n",
      "* **Calculate summary statistics**: Calculate mean, median, mode, standard deviation, and variance for continuous variables.\n",
      "* **Calculate frequency tables**: Calculate frequency tables for categorical variables.\n",
      "* **Visualize the data**: Use plots (e.g., bar chart, box plot) to visualize the data.\n",
      "\n",
      "**III. Understanding Variable Relationships**\n",
      "\n",
      "* **Check for correlations**: Use correlation coefficients (e.g., Pearson's r, Spearman's rho) to check for relationships between variables.\n",
      "* **Check for interactions**: Use interaction terms in regression models to check for interactions between variables.\n",
      "\n",
      "**IV. Data Transformation**\n",
      "\n",
      "* **Check for skewness**: Use plots (e.g., Q-Q plot, histogram) and statistical tests (e.g., Shapiro-Wilk test) to check for skewness.\n",
      "* **Transform the data**: Use transformations (e.g., log transformation, square root transformation) to normalize the data.\n",
      "\n",
      "**V. Choosing the Right Statistical Test**\n",
      "\n",
      "* **Determine the research question**: Determine the research question and the type of analysis needed.\n",
      "* **Choose the right statistical test**: Choose the right statistical test based on the type of data and the research question.\n",
      "\n",
      "**VI. Interpreting Results**\n",
      "\n",
      "* **Interpret the results**: Interpret the results of the statistical analysis in the context of the research question.\n",
      "* **Check for assumptions**: Check for assumptions of the statistical test (e.g., normality, independence).\n",
      "\n",
      "Here are some resources that provide more detailed guidelines:\n",
      "\n",
      "* \"Statistical Analysis\" by UCLA Academic Technology Services\n",
      "* \"Data Analysis\" by Coursera\n",
      "* \"Statistical Inference\" by Stanford University\n",
      "* \"Data Science\" by DataCamp\n",
      "\n",
      "Note: These guidelines are general and may need to be adapted to specific research questions and data types.\n",
      "\n",
      "Here is a bulleted list of key points to remember:\n",
      "\n",
      "* Understand the type of variables and their distributions\n",
      "* Check for normality and skewness\n",
      "* Calculate summary statistics and frequency tables\n",
      "* Visualize the data\n",
      "* Check for correlations and interactions\n",
      "* Choose the right statistical test\n",
      "* Interpret the results in the context of the research question\n",
      "* Check for assumptions of the statistical test\n"
     ]
    }
   ],
   "source": [
    "tool_system_prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: {formatted_date}\n",
    "\"\"\"\n",
    "\n",
    "stats_question = \"Look into the web for guidelines to start a statistical analysis. Like descriptive how to understand the type of variables and what distributions they are following\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\":    \"system\",     \n",
    "     \"content\": system_prompt_content + \"\\nGenerate a bulledpoint guideline\"},\n",
    "    {\"role\":    \"user\",       \"content\": stats_question},\n",
    "  ]\n",
    "#that task should be explicited in the system turn?\n",
    "response_stats = llama31(messages)\n",
    "print(response_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "height": 931
   },
   "source": [
    "Here are some guidelines to start a statistical analysis:\n",
    "\n",
    "**I. Understanding the Data**\n",
    "\n",
    "* **Explore the data**: Look at the distribution of the variables, check for missing values, and identify any outliers.\n",
    "* **Understand the type of variables**: Identify the type of variables you are working with (e.g., continuous, categorical, ordinal).\n",
    "* **Check for normality**: Use plots (e.g., Q-Q plot, histogram) and statistical tests (e.g., Shapiro-Wilk test) to check if the data follows a normal distribution.\n",
    "\n",
    "**II. Descriptive Statistics**\n",
    "\n",
    "* **Calculate summary statistics**: Calculate mean, median, mode, standard deviation, and variance for continuous variables.\n",
    "* **Calculate frequency tables**: Calculate frequency tables for categorical variables.\n",
    "* **Visualize the data**: Use plots (e.g., bar chart, box plot) to visualize the data.\n",
    "\n",
    "**III. Understanding Variable Relationships**\n",
    "\n",
    "* **Check for correlations**: Use correlation coefficients (e.g., Pearson's r, Spearman's rho) to check for relationships between variables.\n",
    "* **Check for interactions**: Use interaction terms in regression models to check for interactions between variables.\n",
    "\n",
    "**IV. Data Transformation**\n",
    "\n",
    "* **Check for skewness**: Use plots (e.g., Q-Q plot, histogram) and statistical tests (e.g., Shapiro-Wilk test) to check for skewness.\n",
    "* **Transform the data**: Use transformations (e.g., log transformation, square root transformation) to normalize the data.\n",
    "\n",
    "**V. Choosing the Right Statistical Test**\n",
    "\n",
    "* **Determine the research question**: Determine the research question and the type of analysis needed.\n",
    "* **Choose the right statistical test**: Choose the right statistical test based on the type of data and the research question.\n",
    "\n",
    "**VI. Interpreting Results**\n",
    "\n",
    "* **Interpret the results**: Interpret the results of the statistical analysis in the context of the research question.\n",
    "* **Check for assumptions**: Check for assumptions of the statistical test (e.g., normality, independence).\n",
    "\n",
    "Here are some resources that provide more detailed guidelines:\n",
    "\n",
    "* \"Statistical Analysis\" by UCLA Academic Technology Services\n",
    "* \"Data Analysis\" by Coursera\n",
    "* \"Statistical Inference\" by Stanford University\n",
    "* \"Data Science\" by DataCamp\n",
    "\n",
    "Note: These guidelines are general and may need to be adapted to specific research questions and data types.\n",
    "\n",
    "Here is a bulleted list of key points to remember:\n",
    "\n",
    "* Understand the type of variables and their distributions\n",
    "* Check for normality and skewness\n",
    "* Calculate summary statistics and frequency tables\n",
    "* Visualize the data\n",
    "* Check for correlations and interactions\n",
    "* Choose the right statistical test\n",
    "* Interpret the results in the context of the research question\n",
    "* Check for assumptions of the statistical test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "height": 200
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Example:**\n",
      "\n",
      "Suppose we are interested in understanding the relationship between the concentration of various pollutants in the air and the rate of respiratory diseases in a given population. We have collected data on the concentration of particulate matter (PM2.5), nitrogen dioxide (NO2), ozone (O3), and carbon monoxide (CO) in the air, as well as the rate of respiratory diseases (RD) in the population.\n",
      "\n",
      "The data is collected from various monitoring stations across the city, and we want to understand how the concentration of these pollutants affects the rate of respiratory diseases.\n",
      "\n",
      "**Dataset:**\n",
      "\n",
      "I will generate a synthetic dataset for this example. The dataset will consist of 1000 observations, each representing a monitoring station. The variables in the dataset are:\n",
      "\n",
      "* PM2.5: concentration of particulate matter (μg/m3)\n",
      "* NO2: concentration of nitrogen dioxide (ppb)\n",
      "* O3: concentration of ozone (ppb)\n",
      "* CO: concentration of carbon monoxide (ppm)\n",
      "* RD: rate of respiratory diseases (per 100,000 people)\n",
      "\n",
      "The dataset will be generated using a combination of normal, lognormal, and Weibull distributions to simulate real-world data.\n",
      "\n",
      "**Synthetic Dataset (.csv file):**\n",
      "\n",
      "Here is a sample of the dataset:\n",
      "\n",
      "| PM2.5 | NO2 | O3 | CO | RD |\n",
      "| --- | --- | --- | --- | --- |\n",
      "| 10.2 | 20.5 | 30.1 | 5.6 | 120.1 |\n",
      "| 12.1 | 22.3 | 31.4 | 6.2 | 130.5 |\n",
      "| 11.5 | 21.9 | 30.7 | 5.9 | 125.3 |\n",
      "| ... | ... | ... | ... | ... |\n",
      "\n",
      "**Documentation:**\n",
      "\n",
      "Here is a brief explanation of the concepts used in this example:\n",
      "\n",
      "* **Particulate Matter (PM2.5)**: PM2.5 refers to particles that are 2.5 micrometers or smaller in diameter. These particles can come from a variety of sources, including vehicle exhaust, industrial activities, and natural sources like dust and pollen.\n",
      "* **Nitrogen Dioxide (NO2)**: NO2 is a gas that is produced by the burning of fossil fuels, such as gasoline and diesel fuel. It can also come from industrial processes and natural sources like lightning.\n",
      "* **Ozone (O3)**: O3 is a gas that is produced when nitrogen oxides and volatile organic compounds react in the presence of sunlight. It can also come from industrial processes and natural sources like lightning.\n",
      "* **Carbon Monoxide (CO)**: CO is a gas that is produced by the incomplete burning of fossil fuels, such as gasoline and diesel fuel. It can also come from industrial processes and natural sources like wildfires.\n",
      "* **Rate of Respiratory Diseases (RD)**: RD refers to the number of cases of respiratory diseases, such as asthma and chronic obstructive pulmonary disease (COPD), per 100,000 people.\n",
      "\n",
      "**Statistical Analysis:**\n",
      "\n",
      "To understand the relationship between the concentration of pollutants and the rate of respiratory diseases, we can perform a multiple linear regression analysis. This will allow us to control for the effects of multiple pollutants on the rate of respiratory diseases.\n",
      "\n",
      "We can also perform a principal component analysis (PCA) to reduce the dimensionality of the data and identify patterns in the relationships between the pollutants.\n",
      "\n",
      "**Code:**\n",
      "\n",
      "Here is some sample code in Python to perform the analysis:\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "# Load the dataset\n",
      "df = pd.read_csv('pollutants.csv')\n",
      "\n",
      "# Perform PCA to reduce dimensionality\n",
      "pca = PCA(n_components=2)\n",
      "pca_df = pca.fit_transform(df[['PM2.5', 'NO2', 'O3', 'CO']])\n",
      "\n",
      "# Perform multiple linear regression\n",
      "X = pca_df\n",
      "y = df['RD']\n",
      "model = LinearRegression()\n",
      "model.fit(X, y)\n",
      "\n",
      "# Print the coefficients\n",
      "print(model.coef_)\n",
      "```\n",
      "This code performs PCA to reduce the dimensionality of the data and then performs multiple linear regression to understand the relationship between the concentration of pollutants and the rate of respiratory diseases.\n",
      "\n",
      "Note: This is just a sample code and may need to be modified to suit the specific needs of the analysis.\n",
      "\n",
      "**Real-World Dataset:**\n",
      "\n",
      "If you prefer to use a real-world dataset, here are a few options:\n",
      "\n",
      "* Air Quality Index (AQI) dataset from the United States Environmental Protection Agency (EPA)\n",
      "* Air pollution dataset from the World Health Organization (WHO)\n",
      "* Respiratory disease dataset from the Centers for Disease Control and Prevention (CDC)\n",
      "\n",
      "Please note that these datasets may require additional processing and cleaning before they can be used for analysis.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\",     \n",
    "     \"content\": system_prompt_content + \"\\nGenerate the text in Markdown format\"},\n",
    "    {\"role\": \"user\", \"content\": stats_question},\n",
    "    {\"role\": \"assistant\", \"content\": response_stats},\n",
    "    {\"role\": \"user\", \"content\": \"Generate a complex example (not typical distributions) to apply this in. Also, create documentation for all the concepts used. Provide me either with a synthetic .csv file made by your own or a real-world dataset from the Internet\" }\n",
    "  ]\n",
    "response = llama31(messages, 405)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "height": 30
   },
   "source": [
    "**Example:**\n",
    "\n",
    "Suppose we are interested in understanding the relationship between the concentration of various pollutants in the air and the rate of respiratory diseases in a given population. We have collected data on the concentration of particulate matter (PM2.5), nitrogen dioxide (NO2), ozone (O3), and carbon monoxide (CO) in the air, as well as the rate of respiratory diseases (RD) in the population.\n",
    "\n",
    "The data is collected from various monitoring stations across the city, and we want to understand how the concentration of these pollutants affects the rate of respiratory diseases.\n",
    "\n",
    "**Dataset:**\n",
    "\n",
    "I will generate a synthetic dataset for this example. The dataset will consist of 1000 observations, each representing a monitoring station. The variables in the dataset are:\n",
    "\n",
    "* PM2.5: concentration of particulate matter (μg/m3)\n",
    "* NO2: concentration of nitrogen dioxide (ppb)\n",
    "* O3: concentration of ozone (ppb)\n",
    "* CO: concentration of carbon monoxide (ppm)\n",
    "* RD: rate of respiratory diseases (per 100,000 people)\n",
    "\n",
    "The dataset will be generated using a combination of normal, lognormal, and Weibull distributions to simulate real-world data.\n",
    "\n",
    "**Synthetic Dataset (.csv file):**\n",
    "\n",
    "Here is a sample of the dataset:\n",
    "\n",
    "| PM2.5 | NO2 | O3 | CO | RD |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 10.2 | 20.5 | 30.1 | 5.6 | 120.1 |\n",
    "| 12.1 | 22.3 | 31.4 | 6.2 | 130.5 |\n",
    "| 11.5 | 21.9 | 30.7 | 5.9 | 125.3 |\n",
    "| ... | ... | ... | ... | ... |\n",
    "\n",
    "**Documentation:**\n",
    "\n",
    "Here is a brief explanation of the concepts used in this example:\n",
    "\n",
    "* **Particulate Matter (PM2.5)**: PM2.5 refers to particles that are 2.5 micrometers or smaller in diameter. These particles can come from a variety of sources, including vehicle exhaust, industrial activities, and natural sources like dust and pollen.\n",
    "* **Nitrogen Dioxide (NO2)**: NO2 is a gas that is produced by the burning of fossil fuels, such as gasoline and diesel fuel. It can also come from industrial processes and natural sources like lightning.\n",
    "* **Ozone (O3)**: O3 is a gas that is produced when nitrogen oxides and volatile organic compounds react in the presence of sunlight. It can also come from industrial processes and natural sources like lightning.\n",
    "* **Carbon Monoxide (CO)**: CO is a gas that is produced by the incomplete burning of fossil fuels, such as gasoline and diesel fuel. It can also come from industrial processes and natural sources like wildfires.\n",
    "* **Rate of Respiratory Diseases (RD)**: RD refers to the number of cases of respiratory diseases, such as asthma and chronic obstructive pulmonary disease (COPD), per 100,000 people.\n",
    "\n",
    "**Statistical Analysis:**\n",
    "\n",
    "To understand the relationship between the concentration of pollutants and the rate of respiratory diseases, we can perform a multiple linear regression analysis. This will allow us to control for the effects of multiple pollutants on the rate of respiratory diseases.\n",
    "\n",
    "We can also perform a principal component analysis (PCA) to reduce the dimensionality of the data and identify patterns in the relationships between the pollutants.\n",
    "\n",
    "**Code:**\n",
    "\n",
    "Here is some sample code in Python to perform the analysis:\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('pollutants.csv')\n",
    "\n",
    "# Perform PCA to reduce dimensionality\n",
    "pca = PCA(n_components=2)\n",
    "pca_df = pca.fit_transform(df[['PM2.5', 'NO2', 'O3', 'CO']])\n",
    "\n",
    "# Perform multiple linear regression\n",
    "X = pca_df\n",
    "y = df['RD']\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Print the coefficients\n",
    "print(model.coef_)\n",
    "```\n",
    "This code performs PCA to reduce the dimensionality of the data and then performs multiple linear regression to understand the relationship between the concentration of pollutants and the rate of respiratory diseases.\n",
    "\n",
    "Note: This is just a sample code and may need to be modified to suit the specific needs of the analysis.\n",
    "\n",
    "**Real-World Dataset:**\n",
    "\n",
    "If you prefer to use a real-world dataset, here are a few options:\n",
    "\n",
    "* Air Quality Index (AQI) dataset from the United States Environmental Protection Agency (EPA)\n",
    "* Air pollution dataset from the World Health Organization (WHO)\n",
    "* Respiratory disease dataset from the Centers for Disease Control and Prevention (CDC)\n",
    "\n",
    "Please note that these datasets may require additional processing and cleaning before they can be used for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "#This is where I customize the tool calling capability to a real-world scenario of my interest\n",
    "#WIP : extract the information from a single book (free copyright) = (https://hastie.su.domains/ISLP/ISLP_website.pdf.download.html)\n",
    "#WIP: I want to custom this tool calling to look into the linkedIn posts of a very well-know statistician that provides valuables insights on statistical methods"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Lab 7: Llama Stack"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 2,
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "_ = load_dotenv() #loads 'TOGETHER_API_KEY'"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 3,
   "source": "#!pip install llama-stack==0.1.0 llama-stack-client==0.1.0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-----------------------------------------------------------------------------+\r\n",
      "\u001B[1m\u001B[97m| Template Name                | Description                                                                 |\u001B[0m\r\n",
      "+------------------------------+-----------------------------------------------------------------------------+\r\n",
      "| bedrock                      | Use AWS Bedrock for running LLM inference and safety                        |\r\n",
      "+------------------------------+-----------------------------------------------------------------------------+\r\n",
      "| cerebras                     | Use Cerebras for running LLM inference                                      |\r\n",
      "+------------------------------+-----------------------------------------------------------------------------+\r\n",
      "| experimental-post-training   | Experimental template for post training                                     |\r\n",
      "+------------------------------+-----------------------------------------------------------------------------+\r\n",
      "| fireworks                    | Use Fireworks.AI for running LLM inference                                  |\r\n",
      "+------------------------------+-----------------------------------------------------------------------------+\r\n",
      "| hf-endpoint                  | Use (an external) Hugging Face Inference Endpoint for running LLM inference |\r\n",
      "+------------------------------+-----------------------------------------------------------------------------+\r\n",
      "| hf-serverless                | Use (an external) Hugging Face Inference Endpoint for running LLM inference |\r\n",
      "+------------------------------+-----------------------------------------------------------------------------+\r\n",
      "| meta-reference-gpu           | Use Meta Reference for running LLM inference                                |\r\n",
      "+------------------------------+-----------------------------------------------------------------------------+\r\n",
      "| meta-reference-quantized-gpu | Use Meta Reference with fp8, int4 quantization for running LLM inference    |\r\n",
      "+------------------------------+-----------------------------------------------------------------------------+\r\n",
      "| nvidia                       | Use NVIDIA NIM for running LLM inference                                    |\r\n",
      "+------------------------------+-----------------------------------------------------------------------------+\r\n",
      "| ollama                       | Use (an external) Ollama server for running LLM inference                   |\r\n",
      "+------------------------------+-----------------------------------------------------------------------------+\r\n",
      "| remote-vllm                  | Use (an external) vLLM server for running LLM inference                     |\r\n",
      "+------------------------------+-----------------------------------------------------------------------------+\r\n",
      "| sambanova                    | Use SambaNova.AI for running LLM inference                                  |\r\n",
      "+------------------------------+-----------------------------------------------------------------------------+\r\n",
      "| tgi                          | Use (an external) TGI server for running LLM inference                      |\r\n",
      "+------------------------------+-----------------------------------------------------------------------------+\r\n",
      "| together                     | Use Together.AI for running LLM inference                                   |\r\n",
      "+------------------------------+-----------------------------------------------------------------------------+\r\n",
      "| vllm-gpu                     | Use a built-in vLLM engine for running LLM inference                        |\r\n",
      "+------------------------------+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "execution_count": 4,
   "source": "!llama stack build --list-templates"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\r\n",
      "\u001B[1m\u001B[97m| API               |\u001B[0m\r\n",
      "+-------------------+\r\n",
      "| inference         |\r\n",
      "+-------------------+\r\n",
      "| safety            |\r\n",
      "+-------------------+\r\n",
      "| agents            |\r\n",
      "+-------------------+\r\n",
      "| vector_io         |\r\n",
      "+-------------------+\r\n",
      "| datasetio         |\r\n",
      "+-------------------+\r\n",
      "| scoring           |\r\n",
      "+-------------------+\r\n",
      "| eval              |\r\n",
      "+-------------------+\r\n",
      "| post_training     |\r\n",
      "+-------------------+\r\n",
      "| tool_runtime      |\r\n",
      "+-------------------+\r\n",
      "| telemetry         |\r\n",
      "+-------------------+\r\n",
      "| models            |\r\n",
      "+-------------------+\r\n",
      "| shields           |\r\n",
      "+-------------------+\r\n",
      "| vector_dbs        |\r\n",
      "+-------------------+\r\n",
      "| datasets          |\r\n",
      "+-------------------+\r\n",
      "| scoring_functions |\r\n",
      "+-------------------+\r\n",
      "| eval_tasks        |\r\n",
      "+-------------------+\r\n",
      "| tool_groups       |\r\n",
      "+-------------------+\r\n",
      "| inspect           |\r\n",
      "+-------------------+\r\n"
     ]
    }
   ],
   "execution_count": 5,
   "source": "!llama stack list-apis"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! You can now use the Llama Stack Client CLI with endpoint https://llama-stack.together.ai\r\n"
     ]
    }
   ],
   "execution_count": 6,
   "source": "!llama-stack-client configure --endpoint https://llama-stack.together.ai"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\r\n",
      "┃\u001B[1m \u001B[0m\u001B[1midentifier   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mprovider_id   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mprovider_res…\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mmetadata      \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mmodel_type\u001B[0m\u001B[1m \u001B[0m┃\r\n",
      "┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\r\n",
      "│ meta-llama/L… │ together       │ meta-llama/M… │ {}             │ llm        │\r\n",
      "│ meta-llama/L… │ together       │ meta-llama/M… │ {}             │ llm        │\r\n",
      "│ meta-llama/L… │ together       │ meta-llama/M… │ {}             │ llm        │\r\n",
      "│ meta-llama/L… │ together       │ meta-llama/L… │ {}             │ llm        │\r\n",
      "│ meta-llama/L… │ together       │ meta-llama/L… │ {}             │ llm        │\r\n",
      "│ meta-llama/L… │ together       │ meta-llama/L… │ {}             │ llm        │\r\n",
      "│ meta-llama/L… │ together       │ meta-llama/L… │ {}             │ llm        │\r\n",
      "│ meta-llama/L… │ together       │ meta-llama/M… │ {}             │ llm        │\r\n",
      "│ meta-llama/L… │ together       │ meta-llama/L… │ {}             │ llm        │\r\n",
      "│ all-MiniLM-L… │ sentence-tran… │ all-MiniLM-L… │ {'embedding_d… │ embedding  │\r\n",
      "│               │                │               │ 384.0}         │            │\r\n",
      "└───────────────┴────────────────┴───────────────┴────────────────┴────────────┘\r\n"
     ]
    }
   ],
   "execution_count": 7,
   "source": "!llama-stack-client models list"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model(identifier='meta-llama/Llama-3.1-8B-Instruct', metadata={}, api_model_type='llm', provider_id='together', provider_resource_id='meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo', type='model', model_type='llm'), Model(identifier='meta-llama/Llama-3.1-70B-Instruct', metadata={}, api_model_type='llm', provider_id='together', provider_resource_id='meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo', type='model', model_type='llm'), Model(identifier='meta-llama/Llama-3.1-405B-Instruct-FP8', metadata={}, api_model_type='llm', provider_id='together', provider_resource_id='meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo', type='model', model_type='llm'), Model(identifier='meta-llama/Llama-3.2-3B-Instruct', metadata={}, api_model_type='llm', provider_id='together', provider_resource_id='meta-llama/Llama-3.2-3B-Instruct-Turbo', type='model', model_type='llm'), Model(identifier='meta-llama/Llama-3.2-11B-Vision-Instruct', metadata={}, api_model_type='llm', provider_id='together', provider_resource_id='meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo', type='model', model_type='llm'), Model(identifier='meta-llama/Llama-3.2-90B-Vision-Instruct', metadata={}, api_model_type='llm', provider_id='together', provider_resource_id='meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo', type='model', model_type='llm'), Model(identifier='meta-llama/Llama-3.3-70B-Instruct', metadata={}, api_model_type='llm', provider_id='together', provider_resource_id='meta-llama/Llama-3.3-70B-Instruct-Turbo', type='model', model_type='llm'), Model(identifier='meta-llama/Llama-Guard-3-8B', metadata={}, api_model_type='llm', provider_id='together', provider_resource_id='meta-llama/Meta-Llama-Guard-3-8B', type='model', model_type='llm'), Model(identifier='meta-llama/Llama-Guard-3-11B-Vision', metadata={}, api_model_type='llm', provider_id='together', provider_resource_id='meta-llama/Llama-Guard-3-11B-Vision-Turbo', type='model', model_type='llm'), Model(identifier='all-MiniLM-L6-v2', metadata={'embedding_dimension': 384.0}, api_model_type='embedding', provider_id='sentence-transformers', provider_resource_id='all-MiniLM-L6-v2', type='model', model_type='embedding')]\n"
     ]
    }
   ],
   "execution_count": 8,
   "source": [
    "import os\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "client = LlamaStackClient(base_url=f\"https://llama-stack.together.ai\")\n",
    "\n",
    "models = client.models.list()\n",
    "print(models)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#  Llama Stack Inference"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "LLAMA_STACK_API_TOGETHER_URL=\"https://llama-stack.together.ai\"\n",
    "LLAMA31_8B_INSTRUCT = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "from llama_stack_client import LlamaStackClient\n",
    "import json\n",
    "\n",
    "def run_main():\n",
    "    client = LlamaStackClient(\n",
    "        base_url=LLAMA_STACK_API_TOGETHER_URL,\n",
    "    )\n",
    "\n",
    "    response = client.inference.chat_completion(\n",
    "        model_id=LLAMA31_8B_INSTRUCT,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Who wrote the book Innovator's Dilemma? How about Charlotte's Web?\"},\n",
    "            {\"role\": \"user\", \"content\": \"which book was published first?\"}            \n",
    "        ],\n",
    "        x_llama_stack_provider_data=json.dumps({\"together_api_key\": os.getenv('TOGETHER_API_KEY')})\n",
    "    )\n",
    "\n",
    "    print(response.completion_message.content)\n",
    "    \n",
    "run_main()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Llama Stack Agent"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.lib.agents.agent import Agent\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "from llama_stack_client.types.agent_create_params import AgentConfig"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "async def run_main():\n",
    "    client = LlamaStackClient(\n",
    "        base_url=LLAMA_STACK_API_TOGETHER_URL,\n",
    "    )    \n",
    "    \n",
    "    agent_config = AgentConfig(\n",
    "        model=LLAMA31_8B_INSTRUCT,\n",
    "        instructions=\"You are a helpful assistant\",\n",
    "        enable_session_persistence=False,\n",
    "    )\n",
    "\n",
    "    agent = Agent(client, agent_config)\n",
    "    session_id = agent.create_session(\"test-session\")\n",
    "\n",
    "    prompts = [\n",
    "        \"Who wrote the book Charlotte's Web?\",\n",
    "        \"Three best quotes?\",\n",
    "    ]\n",
    "\n",
    "    for prompt in prompts:\n",
    "        print(f\"User> {prompt}\")\n",
    "        response = agent.create_turn(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            session_id=session_id,\n",
    "        )\n",
    "\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "        \n",
    "await run_main()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Llama Stack with Llama 3.2 vision model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_image(path):\n",
    "  img = Image.open(path)\n",
    "  plt.imshow(img)\n",
    "  plt.axis('off')\n",
    "  plt.show()\n",
    "\n",
    "display_image(\"./content/Llama_Repo.jpeg\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "LLAMA32_11B_INSTRUCT = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "import base64\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        base64_string = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "        base64_url = f\"data:image/png;base64,{base64_string}\"\n",
    "        return base64_url\n",
    "\n",
    "async def run_main(image_path, prompt):\n",
    "    base64_image = encode_image(image_path)\n",
    "    \n",
    "    client = LlamaStackClient(\n",
    "        base_url=LLAMA_STACK_API_TOGETHER_URL,\n",
    "    )    \n",
    "    \n",
    "    agent_config = AgentConfig(\n",
    "        model=LLAMA32_11B_INSTRUCT,\n",
    "        instructions=\"You are a helpful assistant\",\n",
    "        enable_session_persistence=False,\n",
    "    )\n",
    "\n",
    "    agent = Agent(client, agent_config)\n",
    "    session_id = agent.create_session(\"test-session\")\n",
    "\n",
    "    response = agent.create_turn(\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": {\n",
    "                         \"url\": {\n",
    "                              \"uri\": encode_image(image_path)\n",
    "                         }\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt,\n",
    "                }\n",
    "            ]\n",
    "        }],\n",
    "        session_id=session_id,\n",
    "    )\n",
    "\n",
    "    for log in EventLogger().log(response):\n",
    "        log.print()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "await run_main(\"./content/Llama_Repo.jpeg\",\n",
    "         \"How many different colors are those llamas? What are those colors?\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import mimetypes\n",
    "from termcolor import cprint\n",
    "from llama_stack_client.lib.inference.event_logger import EventLogger\n",
    "\n",
    "async def run_main(image_path: str, prompt):\n",
    "    client = LlamaStackClient(\n",
    "        base_url=LLAMA_STACK_API_TOGETHER_URL,\n",
    "    )    \n",
    "\n",
    "    message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": {\n",
    "                     \"url\": {\n",
    "                          \"uri\": encode_image(image_path)\n",
    "                     }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt,\n",
    "            }\n",
    "        ]       \n",
    "    }\n",
    "\n",
    "    cprint(\"User> Sending image for analysis...\", \"green\")\n",
    "    response = client.inference.chat_completion(\n",
    "        messages=[message],\n",
    "        model_id=LLAMA32_11B_INSTRUCT,\n",
    "        stream=False,\n",
    "    )\n",
    "\n",
    "    print(response.completion_message.content.lower().strip())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "await run_main(\"./content/Llama_Repo.jpeg\",\n",
    "     \"How many different colors are those llamas?\\\n",
    "     What are those colors?\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The await keyword is used within an asynchronous function to pause its execution until a specified coroutine has completed. \n",
    "\n",
    "This essential concept in asynchronous programming allows Python to yield control back to the event loop, enabling other asynchronous tasks to run while the awaited coroutine is in progress. With await, we can effectively handle multiple I/O-bound (Input-Output) operations concurrently, leading to more responsive and efficient code.\n",
    "\n",
    "A coroutine is a regular function with the ability to pause its execution when encountering an operation that may take a while to complete.\n",
    "The async keyword creates a coroutine."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
