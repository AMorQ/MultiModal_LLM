{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa43f487-7649-486c-8818-13134a36cc1f",
   "metadata": {},
   "source": "# Tokenization"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### WIP: Do a spanish version: \n",
    "- https://huggingface.co/DeepESP/gpt2-spanish\n",
    "- https://spacy.io/models/es/ (https://stackoverflow.com/questions/42947733/use-spacy-spanish-tokenizer)\n",
    "- https://github.com/chriskhanhtran/spanish-bert\n",
    "- https://www.kdnuggets.com/how-to-create-a-custom-tokenizer-for-non-english-languages-with-hugging-face-transformers\n",
    "\n",
    "\n",
    "\n",
    "    (python) libraries\n",
    "    - NLTK --> spaCy --> HuggingFace Transformers"
   ],
   "id": "aa263526ea8b33d8"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "516d8dc0-88c8-4f5b-976a-953239876e1d",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56ba58f-d67b-47ee-8597-5bee85e987ce",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "code",
   "id": "10adcf23-3f2b-4df7-85cb-6808cb9ed394",
   "metadata": {
    "height": 948,
    "ExecuteTime": {
     "end_time": "2025-04-13T14:57:15.086845Z",
     "start_time": "2025-04-13T14:57:13.758084Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import tiktoken #new tokenizer in the LLama models\n",
    "from tiktoken.load import load_tiktoken_bpe #bpe = byte-pair encoding\n",
    "import torch\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ],
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found. Error loading \"C:\\Users\\amorc\\PycharmProjects\\MultiModal_Llama\\venvs\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mOSError\u001B[39m                                   Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtiktoken\u001B[39;00m \u001B[38;5;66;03m#new tokenizer and expanded vocabulary\u001B[39;00m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtiktoken\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mload\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m load_tiktoken_bpe\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mjson\u001B[39;00m\n\u001B[32m      8\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmatplotlib\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpyplot\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mplt\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MultiModal_Llama\\venvs\\Lib\\site-packages\\torch\\__init__.py:148\u001B[39m\n\u001B[32m    146\u001B[39m                 err = ctypes.WinError(ctypes.get_last_error())\n\u001B[32m    147\u001B[39m                 err.strerror += \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m Error loading \u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdll\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33m or one of its dependencies.\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m148\u001B[39m                 \u001B[38;5;28;01mraise\u001B[39;00m err\n\u001B[32m    150\u001B[39m     kernel32.SetErrorMode(prev_error_mode)\n\u001B[32m    153\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_preload_cuda_deps\u001B[39m(lib_folder, lib_name):\n",
      "\u001B[31mOSError\u001B[39m: [WinError 126] The specified module could not be found. Error loading \"C:\\Users\\amorc\\PycharmProjects\\MultiModal_Llama\\venvs\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies."
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tiktoken tokenizer\n",
    "\n",
    "#Tiktoken is a library for tokenizing text, specifically designed for use with OpenAI's GPT-3 and other transformer models. It provides a fast and efficient way to convert text into tokens that can be processed by these models.\n",
    "\n",
    "Generates a vocabulary of 128000 tokens. \n",
    "    HUMAN BEINGS EXCEL IN LANGUAGE WITH JUST ABOUT 50.000!!!!\n",
    "    \n",
    "PARAMETERS (WHEN ENCODING WITH TIKTOKEN):\n",
    "\n",
    "    pat_str = regex pattern that defines how text should be split into tokens\n",
    "    mergeable_ranks = dict that assigns ranking to different tokens (efficient token merging) based on how frequently they appear. It is used by Byte Pair Encoding (BPE) or similar tokenization techniques to efficiently merge subwords into full words\n",
    "               it's a dict with keys as strings (tokens) and values as integers (ranks):\n",
    "               - Keys are byte sequences representing tokens (shown as b'...\\x' because they're raw bytes that may include non-printable characters-many tokens include special characters, control codes, or UTF-8 encoded characters that don't have a simple text representation.)\n",
    "               - Values are integers representing the rank/ID of each token in the vocabulary\n",
    "               \n",
    "                This mapping is crucial for tokenization as it determines:  \n",
    "                - How text is split into tokens\n",
    "                - Which byte sequences are considered single tokens\n",
    "                - The priority of merging byte sequences based on their frequency in the training data\n",
    " \n",
    "    special_tokens = list of special tokens that have specific meanings in the context of the model (e.g., start/end of text, padding, etc.)\n",
    "    num_reserved_special_tokens = number of special tokens that are reserved for future use or specific purposes\n",
    "    name = name of the tokenizer model (used for identification)\n",
    "     \n",
    "\n",
    "- COMMON WORDS LIKE 'THE ' WILL HAVE A LOWER RANKING BECAUSE THEY APPEAR FREQUENTLY AND THUS, IT IS EXPECTED THAT THEY DON´T CONVEY THAT MUCH MEANING JUST SYNTAX\n"
   ],
   "id": "b16f787263fd8f28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T15:42:39.948449Z",
     "start_time": "2025-04-13T15:42:38.762997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Initialize tiktoken tokenizer\n",
    "tokenizer_path = \"./content/tokenizer.model\" #column of tokens in the format subword == token\n",
    "\n",
    "num_reserved_special_tokens = 256\n",
    "\n",
    "mergeable_ranks = load_tiktoken_bpe(tokenizer_path) \n",
    "\n",
    "num_base_tokens = len(mergeable_ranks)\n",
    "special_tokens = [\n",
    "    \"<|begin_of_text|>\",\n",
    "    \"<|end_of_text|>\",\n",
    "    \"<|reserved_special_token_0|>\",\n",
    "    \"<|reserved_special_token_1|>\",\n",
    "    \"<|finetune_right_pad_id|>\", #???\n",
    "    \"<|step_id|>\",\n",
    "    \"<|start_header_id|>\",\n",
    "    \"<|end_header_id|>\",\n",
    "    \"<|eom_id|>\",\n",
    "    \"<|eot_id|>\",\n",
    "    \"<|python_tag|>\",\n",
    "]\n",
    "reserved_tokens = [\n",
    "    f\"<|reserved_special_token_{2 + i}|>\"\n",
    "    for i in range(num_reserved_special_tokens - len(special_tokens))\n",
    "]\n",
    "special_tokens = special_tokens + reserved_tokens\n",
    "\n",
    "# source: https://github.com/meta-llama/llama-models/blob/main/models/llama3/api/tokenizer.py#L53\n",
    "tokenizer = tiktoken.Encoding(\n",
    "    name=Path(tokenizer_path).name,#name of the file with the tokenizer model\n",
    "    pat_str=r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\",\n",
    "    mergeable_ranks=mergeable_ranks,\n",
    "    special_tokens={token: len(mergeable_ranks) + i for i, token in enumerate(special_tokens)},\n",
    "    #define 256 more tokens (special ones). They will be dedicated to special characters/behaviours\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "4c08872aae34fc7a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65bbd561",
   "metadata": {
    "height": 149
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!pip install tokenizer\\nimport tokenizer\\ntokenizer.save(\"./content/tokenizer_model_spanish.json\")\\n#tokenizer_path_spanish = \"./content/tokenizer_model_spanish.json\"\\n#mergeable_ranks_spanish = load_tiktoken_bpe(tokenizer_path_spanish)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "!pip install tokenizer\n",
    "import tokenizer\n",
    "tokenizer.save(\"./content/tokenizer_model_spanish.json\")\n",
    "#tokenizer_path_spanish = \"./content/tokenizer_model_spanish.json\"\n",
    "#mergeable_ranks_spanish = load_tiktoken_bpe(tokenizer_path_spanish)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91afd22a",
   "metadata": {
    "height": 251
   },
   "source": [
    "\n",
    "\n",
    "```python\n",
    "tokenizer_spanish = tiktoken.Encoding(\n",
    "    name=Path(tokenizer_path).name + '_spanish',#name of the file with the tokenizer model\n",
    "    pat_str=r\"(?i:\\b[a-záéíóúüñ]+['’]?[a-záéíóúüñ]*\\b) | \\b\\d{1,3}(\\.\\d{3})*(,\\d+)?\\b |\\b\\d+(\\.\\d+)?\\b |[¿¡!?,;:.\\\"“”(){}\\[\\]] |\\s+\",\n",
    "    mergeable_ranks=mergeable_ranks,\n",
    "    special_tokens={token: len(mergeable_ranks) + i for i, token in enumerate(special_tokens)},\n",
    "    #define 256 more tokens (special ones). They will be dedicated to special characters/behaviours\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3e2b37",
   "metadata": {
    "height": 132
   },
   "source": [
    "\n",
    "## Regex Pattern in Spanish\n",
    "r\"(?i:\\b[a-záéíóúüñ]+['’]?[a-záéíóúüñ]*\\b) |  # Words (with optional apostrophe for foreign words)\n",
    "\n",
    "   \\b\\d{1,3}(\\.\\d{3})*(,\\d+)?\\b |             # Numbers with thousands separators (1.000, 20.000, 1.234,56)\n",
    "   \n",
    "   \\b\\d+(\\.\\d+)?\\b |                           # Numbers with decimal points (3.14, 123.45)\n",
    "   \n",
    "   [¿¡!?,;:.\\\"“”(){}\\[\\]] |                    # Punctuation (including Spanish-specific ¿¡)\n",
    "   \n",
    "   \\s+\"                                        # Whitespace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c083125",
   "metadata": {
    "height": 149
   },
   "source": [
    "## Regex Pattern for token Optimization\n",
    "### This regex ensures that:\n",
    "\n",
    "    Words are properly separated.\n",
    "    Numbers are split correctly.\n",
    "    Punctuation is handled separately.\n",
    "    Spaces and newlines are treated correctly.\n",
    "\n",
    "\n",
    "r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|\n",
    "- Matches common English contractions like \"I'm\", \"you're\", \"he's\", \"we've\", \"it'll\", \"I'd\".\n",
    "- (?i:) makes it case-insensitive.\n",
    "\n",
    "[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\n",
    "- Matches words (sequences of letters).\n",
    "- \\p{L} refers to Unicode letters.\n",
    "- \\p{N} refers to Unicode numbers.\n",
    "- [^\\r\\n\\p{L}\\p{N}]? ensures that a word may have a leading punctuation character (like \"hello!\" still being \"hello\").\n",
    "   \n",
    "\\p{N}{1,3}|\n",
    "- Matches numbers of length 1 to 3 (like \"1\", \"23\", \"456\" but not \"1234\").\n",
    "- This helps in efficient tokenization of numbers.\n",
    "\n",
    "?[^\\s\\p{L}\\p{N}]+[\\r\\n] * |\n",
    "- Matches punctuation and symbols that are not letters or numbers.\n",
    "- [^\\s\\p{L}\\p{N}]+ ensures we capture symbols separately (e.g., \"!\", \"@\", \"#$%\").\n",
    "- [\\r\\n]* allows it to optionally match line breaks.\n",
    "    \n",
    "\\s*[\\r\\n]+|\n",
    "- Matches newlines and surrounding spaces (\\r\\n refers to line breaks).\n",
    "\\s+(?!\\S)|\n",
    "\n",
    "- Matches whitespace sequences that are at the end of a line (avoids capturing spaces that are part of words).\n",
    "\n",
    "\\s+\"\n",
    "- Matches general whitespace (spaces, tabs, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "id": "190fa04b-af53-401d-a0eb-1fd9f03a2dad",
   "metadata": {
    "height": 30,
    "ExecuteTime": {
     "end_time": "2025-04-13T15:42:50.483906Z",
     "start_time": "2025-04-13T15:42:50.398368Z"
    }
   },
   "source": [
    "tokenizer.encode(\"hello\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15339]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "302dac11",
   "metadata": {
    "height": 30,
    "ExecuteTime": {
     "end_time": "2025-04-13T15:42:53.761347Z",
     "start_time": "2025-04-13T15:42:53.695638Z"
    }
   },
   "source": "#tokenizer_spanish.encode('hola')",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer_spanish' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mtokenizer_spanish\u001B[49m.encode(\u001B[33m'\u001B[39m\u001B[33mhola\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'tokenizer_spanish' is not defined"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9f8cb2a-a18b-4daa-9dd1-5f287ad55ffe",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([15339])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9e19f39-0a11-4ad6-921d-69711dffad36",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15339, 13929]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"hello Andrew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9f1186e-9aaf-473a-8e2d-86e55b15d70f",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15339, 323, 4361]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"hello andrew\") # = hello, and, rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ab55ba3",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' and'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([323])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Why LLM are not really reasoning with this setup?",
   "id": "203c5529037a30b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 r's in the word \"strawberry\".\n"
     ]
    }
   ],
   "execution_count": 25,
   "source": [
    "question = \"How many r's in the word strawberry?\"\n",
    "prompt = f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "response = llama31(prompt)\n",
    "print(response)"
   ],
   "id": "9c5f1c368cd83720"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198 \n",
      "\n",
      "128000 <|begin_of_text|>\n",
      "128006 <|start_header_id|>\n",
      "882 user\n",
      "128007 <|end_header_id|>\n",
      "271 \n",
      "\n",
      "\n",
      "4438 How\n",
      "1690  many\n",
      "436  r\n",
      "596 's\n",
      "304  in\n",
      "279  the\n",
      "3492  word\n",
      "73700  strawberry\n",
      "30 ?\n",
      "128009 <|eot_id|>\n",
      "128006 <|start_header_id|>\n",
      "78191 assistant\n",
      "128007 <|end_header_id|>\n",
      "198 \n",
      "\n"
     ]
    }
   ],
   "execution_count": 23,
   "source": [
    "#why is it confused? \n",
    "encoded_tokens = tokenizer.encode(prompt, allowed_special=\"all\")\n",
    "decoded_tokens = [tokenizer.decode([token]) for token in encoded_tokens]\n",
    "\n",
    "for e, d in zip(encoded_tokens, decoded_tokens):\n",
    "    print(e, d)"
   ],
   "id": "afec7c584a8721b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"color: black; background-color: #ADE0FC; padding: 2px;\">\\n</span><span style=\"color: black; background-color: #FCE278; padding: 2px;\"><|begin_of_text|></span><span style=\"color: black; background-color: #B2D1FE; padding: 2px;\"><|start_header_id|></span><span style=\"color: black; background-color: #AFF7C6; padding: 2px;\">user</span><span style=\"color: black; background-color: #FDCE9B; padding: 2px;\"><|end_header_id|></span><span style=\"color: black; background-color: #97F1FB; padding: 2px;\">\\n\\n</span><span style=\"color: black; background-color: #DEE1E7; padding: 2px;\">How</span><span style=\"color: black; background-color: #E3C9FF; padding: 2px;\"> many</span><span style=\"color: black; background-color: #BBC6FD; padding: 2px;\"> r</span><span style=\"color: black; background-color: #D1FB8C; padding: 2px;\">'s</span><span style=\"color: black; background-color: #ADE0FC; padding: 2px;\"> in</span><span style=\"color: black; background-color: #FCE278; padding: 2px;\"> the</span><span style=\"color: black; background-color: #B2D1FE; padding: 2px;\"> word</span><span style=\"color: black; background-color: #AFF7C6; padding: 2px;\"> strawberry</span><span style=\"color: black; background-color: #FDCE9B; padding: 2px;\">?</span><span style=\"color: black; background-color: #97F1FB; padding: 2px;\"><|eot_id|></span><span style=\"color: black; background-color: #DEE1E7; padding: 2px;\"><|start_header_id|></span><span style=\"color: black; background-color: #E3C9FF; padding: 2px;\">assistant</span><span style=\"color: black; background-color: #BBC6FD; padding: 2px;\"><|end_header_id|></span><span style=\"color: black; background-color: #D1FB8C; padding: 2px;\">\\n</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 24,
   "source": "display(HTML(html_tokens(decoded_tokens)))",
   "id": "727d1565a294c354"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 r's in the word \"strawberry\".\n"
     ]
    }
   ],
   "execution_count": 26,
   "source": [
    "question = \"How many r's in the word s t r a w b e r r y? \"\n",
    "prompt = f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "response = llama31(prompt)\n",
    "print(response)"
   ],
   "id": "113575c9931e07b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"color: black; background-color: #ADE0FC; padding: 2px;\">\\n</span><span style=\"color: black; background-color: #FCE278; padding: 2px;\"><|begin_of_text|></span><span style=\"color: black; background-color: #B2D1FE; padding: 2px;\"><|start_header_id|></span><span style=\"color: black; background-color: #AFF7C6; padding: 2px;\">user</span><span style=\"color: black; background-color: #FDCE9B; padding: 2px;\"><|end_header_id|></span><span style=\"color: black; background-color: #97F1FB; padding: 2px;\">\\n\\n</span><span style=\"color: black; background-color: #DEE1E7; padding: 2px;\">How</span><span style=\"color: black; background-color: #E3C9FF; padding: 2px;\"> many</span><span style=\"color: black; background-color: #BBC6FD; padding: 2px;\"> r</span><span style=\"color: black; background-color: #D1FB8C; padding: 2px;\">'s</span><span style=\"color: black; background-color: #ADE0FC; padding: 2px;\"> in</span><span style=\"color: black; background-color: #FCE278; padding: 2px;\"> the</span><span style=\"color: black; background-color: #B2D1FE; padding: 2px;\"> word</span><span style=\"color: black; background-color: #AFF7C6; padding: 2px;\"> s</span><span style=\"color: black; background-color: #FDCE9B; padding: 2px;\"> t</span><span style=\"color: black; background-color: #97F1FB; padding: 2px;\"> r</span><span style=\"color: black; background-color: #DEE1E7; padding: 2px;\"> a</span><span style=\"color: black; background-color: #E3C9FF; padding: 2px;\"> w</span><span style=\"color: black; background-color: #BBC6FD; padding: 2px;\"> b</span><span style=\"color: black; background-color: #D1FB8C; padding: 2px;\"> e</span><span style=\"color: black; background-color: #ADE0FC; padding: 2px;\"> r</span><span style=\"color: black; background-color: #FCE278; padding: 2px;\"> r</span><span style=\"color: black; background-color: #B2D1FE; padding: 2px;\"> y</span><span style=\"color: black; background-color: #AFF7C6; padding: 2px;\">?</span><span style=\"color: black; background-color: #FDCE9B; padding: 2px;\"> </span><span style=\"color: black; background-color: #97F1FB; padding: 2px;\"><|eot_id|></span><span style=\"color: black; background-color: #DEE1E7; padding: 2px;\"><|start_header_id|></span><span style=\"color: black; background-color: #E3C9FF; padding: 2px;\">assistant</span><span style=\"color: black; background-color: #BBC6FD; padding: 2px;\"><|end_header_id|></span><span style=\"color: black; background-color: #D1FB8C; padding: 2px;\">\\n</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 27,
   "source": [
    "encoded_tokens = tokenizer.encode(prompt, allowed_special=\"all\")\n",
    "decoded_tokens = [tokenizer.decode([token]) for token in encoded_tokens]\n",
    "display(HTML(html_tokens(decoded_tokens)))"
   ],
   "id": "25d4307ec4e0560f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The model was confused because its seeing 'strawberry' as a sinlge token and thus, cannot see the individual letters",
   "id": "3222e181dbbac537"
  },
  {
   "cell_type": "markdown",
   "id": "f1b4f561-2e7b-465e-8da0-e0148c631beb",
   "metadata": {
    "id": "InJMbvnb5F1a"
   },
   "source": [
    "## Getting the length of tokens of an input text\n",
    "\n",
    "remember: provides charge based on tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5438a5c5-9602-49d0-b327-6514dd334817",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 47,
    "id": "XTqQQEgq3_12",
    "outputId": "6b8d7489-4877-4c59-8888-518ca95ac2b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"hello world\"\n",
    "len(tokenizer.encode(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab8efbb4-6370-40e4-9615-f1eb597d980c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 149,
    "id": "k85mOcbv4TvL",
    "outputId": "a080269c-0d1f-4c86-da49-d81aaae51bea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Who wrote the book Charlotte's Web?\"\n",
    "prompt = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "encoded_tokens = tokenizer.encode(prompt, allowed_special=\"all\")\n",
    "len(encoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "id": "e4c57fc0-f538-43de-9e82-858ec11b22a4",
   "metadata": {
    "height": 64,
    "ExecuteTime": {
     "end_time": "2025-04-13T15:44:32.926028Z",
     "start_time": "2025-04-13T15:44:32.707226Z"
    }
   },
   "source": [
    "decoded_tokens = [tokenizer.decode([token]) for token in encoded_tokens]\n",
    "for e, d in zip(encoded_tokens, decoded_tokens):\n",
    "    print(e, d)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoded_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m decoded_tokens = [tokenizer.decode([token]) \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m \u001B[43mencoded_tokens\u001B[49m]\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m e, d \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(encoded_tokens, decoded_tokens):\n\u001B[32m      3\u001B[39m     \u001B[38;5;28mprint\u001B[39m(e, d)\n",
      "\u001B[31mNameError\u001B[39m: name 'encoded_tokens' is not defined"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24b60cc2-6167-4eb2-84e2-fb850ae49454",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from utils import html_tokens, llama31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "235d5f1b-e8b9-4507-8785-03a1211cc9be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "height": 30,
    "id": "89FPMUAsBfwG",
    "outputId": "57b2a29c-21f7-401b-e543-298ff34d1219"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"color: black; background-color: #ADE0FC; padding: 2px;\"><|begin_of_text|></span><span style=\"color: black; background-color: #FCE278; padding: 2px;\"><|start_header_id|></span><span style=\"color: black; background-color: #B2D1FE; padding: 2px;\">user</span><span style=\"color: black; background-color: #AFF7C6; padding: 2px;\"><|end_header_id|></span><span style=\"color: black; background-color: #FDCE9B; padding: 2px;\">\\n\\n</span><span style=\"color: black; background-color: #97F1FB; padding: 2px;\">Who</span><span style=\"color: black; background-color: #DEE1E7; padding: 2px;\"> wrote</span><span style=\"color: black; background-color: #E3C9FF; padding: 2px;\"> the</span><span style=\"color: black; background-color: #BBC6FD; padding: 2px;\"> book</span><span style=\"color: black; background-color: #D1FB8C; padding: 2px;\"> Charlotte</span><span style=\"color: black; background-color: #ADE0FC; padding: 2px;\">'s</span><span style=\"color: black; background-color: #FCE278; padding: 2px;\"> Web</span><span style=\"color: black; background-color: #B2D1FE; padding: 2px;\">?</span><span style=\"color: black; background-color: #AFF7C6; padding: 2px;\"><|eot_id|></span><span style=\"color: black; background-color: #FDCE9B; padding: 2px;\"><|start_header_id|></span><span style=\"color: black; background-color: #97F1FB; padding: 2px;\">assistant</span><span style=\"color: black; background-color: #DEE1E7; padding: 2px;\"><|end_header_id|></span><span style=\"color: black; background-color: #E3C9FF; padding: 2px;\">\\n</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(html_tokens(decoded_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e1a205b-c47a-4e24-a75d-2bf7a02a94db",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"color: black; background-color: #ADE0FC; padding: 2px;\">Sup</span><span style=\"color: black; background-color: #FCE278; padding: 2px;\">erc</span><span style=\"color: black; background-color: #B2D1FE; padding: 2px;\">al</span><span style=\"color: black; background-color: #AFF7C6; padding: 2px;\">if</span><span style=\"color: black; background-color: #FDCE9B; padding: 2px;\">rag</span><span style=\"color: black; background-color: #97F1FB; padding: 2px;\">il</span><span style=\"color: black; background-color: #DEE1E7; padding: 2px;\">istic</span><span style=\"color: black; background-color: #E3C9FF; padding: 2px;\">exp</span><span style=\"color: black; background-color: #BBC6FD; padding: 2px;\">ial</span><span style=\"color: black; background-color: #D1FB8C; padding: 2px;\">id</span><span style=\"color: black; background-color: #ADE0FC; padding: 2px;\">ocious</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Try one of you own:\n",
    "prompt = \"Supercalifragilisticexpialidocious\"\n",
    "encoded_tokens = tokenizer.encode(prompt, allowed_special=\"all\")\n",
    "decoded_tokens = [tokenizer.decode([token]) for token in encoded_tokens]\n",
    "display(HTML(html_tokens(decoded_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c72f29-dd4d-41a4-83d4-5205a108283d",
   "metadata": {
    "id": "k-QJtnHBAekO"
   },
   "source": [
    "## Llama 3.1 tokenization model file demystification\n",
    "\n",
    "The Llama 3.1 tokenization model, named as tokenizer.model, can be downloaded along with the Llama 3.1 model weights or from the Llama models repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9269bc84-ba31-418d-92de-53d2ba5770bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 64,
    "id": "nvKQX-E8JgB0",
    "outputId": "77a157ab-1bfb-40f5-e4d9-13eea7cb4465"
   },
   "outputs": [],
   "source": [
    "# download the Llama 3.1 tokenizer model\n",
    "#!wget https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3/api/tokenizer.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6df0569-a8bc-4820-a61a-b16d24da5597",
   "metadata": {
    "id": "fvKOWlWpmE7F"
   },
   "source": [
    "If you take a quick look at the model file, you'll see it has 128,000 lines and each line has two values separated by a space: a mysterious string and a number that starts with 0 and ends with 127,999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8c39d7a-adfc-4394-a919-053b5251809a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "V2w9tshTmt7z",
    "outputId": "6c8def0c-6d49-4de1-d286-33ac2f1754de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IQ== 0\r\n",
      "Ig== 1\r\n",
      "Iw== 2\r\n",
      "JA== 3\r\n",
      "JQ== 4\r\n",
      "Jg== 5\r\n",
      "Jw== 6\r\n",
      "KA== 7\r\n",
      "KQ== 8\r\n",
      "Kg== 9\r\n"
     ]
    }
   ],
   "source": [
    "!head -10 ./content/tokenizer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4142dfd3-d368-4c9c-9f0e-3d6e25872140",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "5V2V0-7Pmt-O",
    "outputId": "fa9a0a55-08a3-4838-af93-fb069a497be5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4LmM4LiB4Lij 127990\r\n",
      "zrbOsQ== 127991\r\n",
      "IOuNlOyasQ== 127992\r\n",
      "2YjZhNin2Ko= 127993\r\n",
      "0LLQsNGC0LjRgdGP 127994\r\n",
      "IGvDtms= 127995\r\n",
      "2YbYqA== 127996\r\n",
      "INCy0YvRgdC+0LrQvtC5 127997\r\n",
      "44O844O8 127998\r\n",
      "6ZSm 127999\r\n"
     ]
    }
   ],
   "source": [
    "!tail -10 ./content/tokenizer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e1815ba-d5fe-4cca-abe6-cc307c7c5598",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "SdqWumbTmuA5",
    "outputId": "6c11fa7a-c7fc-4370-b11c-f06ad4791634"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128000 ./content/tokenizer.model\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l ./content/tokenizer.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0e814c-bf8a-41da-82eb-42c9429de2a7",
   "metadata": {
    "id": "6fHCjwHvm24l"
   },
   "source": [
    "*Each line indeed describes one token out of 128K total tokens and its associated integer ID, and the string on each line is base64 encoded*. Use the code snippet below to decode those 128K encoded strings, and then convert the decoded bytes to more readable UTF-8 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7681feee-b4d0-4fae-a81c-405bd23d2de3",
   "metadata": {
    "height": 234,
    "id": "x89Mdqinb_Cv"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "encoded_tokens = []\n",
    "decoded_byte_tokens = []\n",
    "decoded_utf8_tokens = []\n",
    "\n",
    "with open(\"./content/tokenizer.model\", 'r') as file:\n",
    "  for i, line in enumerate(file):\n",
    "    k, v = line.strip().split(' ')\n",
    "    encoded_tokens.append({k: v})\n",
    "    decoded_byte_tokens.append({base64.b64decode(k): v})\n",
    "    decoded_utf8_tokens.append({base64.b64decode(k).decode('utf-8', errors=\"replace\") : v})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ae39d9-9ee7-45c8-8d71-005b266619c0",
   "metadata": {
    "id": "JfuUILBIoNgc"
   },
   "source": [
    "Let's check the first ten encoded tokens (what's stored in the tokenizer.model), and their decoded byte and UTF-8 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80577ae9-f648-494c-9496-d3c1055c77dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "Q6BrA-5JMfrH",
    "outputId": "34f88ab4-1976-4a32-aded-c83a4248848f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'IQ==': '0'},\n",
       " {'Ig==': '1'},\n",
       " {'Iw==': '2'},\n",
       " {'JA==': '3'},\n",
       " {'JQ==': '4'},\n",
       " {'Jg==': '5'},\n",
       " {'Jw==': '6'},\n",
       " {'KA==': '7'},\n",
       " {'KQ==': '8'},\n",
       " {'Kg==': '9'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(encoded_tokens)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "541c0a61-9bad-4c08-96c9-0cff710227ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "n_FQjyZcMft-",
    "outputId": "637327fa-0733-4562-bf3d-6cff32c389da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{b'!': '0'},\n",
       " {b'\"': '1'},\n",
       " {b'#': '2'},\n",
       " {b'$': '3'},\n",
       " {b'%': '4'},\n",
       " {b'&': '5'},\n",
       " {b\"'\": '6'},\n",
       " {b'(': '7'},\n",
       " {b')': '8'},\n",
       " {b'*': '9'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(decoded_byte_tokens)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9df6456-fbcb-40d0-9468-277eddcf91b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "rqr0ZSPuMfw1",
    "outputId": "616752a5-bbe6-488f-a10a-9fbfa17329b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'!': '0'},\n",
       " {'\"': '1'},\n",
       " {'#': '2'},\n",
       " {'$': '3'},\n",
       " {'%': '4'},\n",
       " {'&': '5'},\n",
       " {\"'\": '6'},\n",
       " {'(': '7'},\n",
       " {')': '8'},\n",
       " {'*': '9'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(decoded_utf8_tokens)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd884a62-13db-420f-8c3f-5d33fa7b51cc",
   "metadata": {
    "id": "clepcO48paeN"
   },
   "source": [
    "Let's confirm the tokenizer.model file stores the base64 encoded strings for tokens, e.g. the token \"hello\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37d9c876-d428-45a8-9ec4-e953ec004d92",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'aA=='"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base64.b64encode('h'.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c304e01e-20e9-4ec6-9acc-1cf19a8be0f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "rcCuoBg_GYOF",
    "outputId": "e8acb422-f239-4bf6-8ce5-79ab776653ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'aGVsbG8='"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base64.b64encode('hello'.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dce9b9ae-3b40-43ee-ba61-24a66cee7d88",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "9M0iDw1jGYQt",
    "outputId": "7cef033f-f345-4eab-dd22-8825a9400c5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aGVsbG8= 15339\r\n"
     ]
    }
   ],
   "source": [
    "!grep \"aGVsbG8=\" ./content/tokenizer.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012c07c0-70f5-4480-9825-c31637306475",
   "metadata": {
    "id": "DygZtn-HBPUu"
   },
   "source": [
    "# More LLM reasoning vs tokenization\n",
    "\n",
    "Let's try out Llama 3.1 on some recent tokenization related LLM problems, and see if we can improve its reasoning by some prompt engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90548f45-c312-40ac-bc86-db8a719979fa",
   "metadata": {
    "id": "QGw3xRYytwG5"
   },
   "source": [
    "## Simple math problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45171fef-8dc5-4636-b36e-c600a35ef8e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 149,
    "id": "p38aw09tQ-Z5",
    "outputId": "aaaf706b-d1ea-4c01-9cc2-2942b51946cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.9 is bigger.\n"
     ]
    }
   ],
   "source": [
    "question = \"Which number is bigger, 9.11 or 9.9? \"\n",
    "prompt = f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "response = llama31(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ad3466d-9f02-41da-bc39-490bd4bcf628",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 47,
    "id": "gsjH4XSntLwO",
    "outputId": "f705ac59-a0a8-4412-8b10-93c6b03e08d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.9 is bigger than 9.11.\n"
     ]
    }
   ],
   "source": [
    "response = llama31(prompt, 70)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c902284b-af14-427f-8bba-e8ed13da2e75",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 47,
    "id": "wLt3-k05tLzR",
    "outputId": "29e2107e-2f24-4c37-d504-e7fe5826d443"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number 9.11 is bigger than 9.9.\n",
      "\n",
      "To compare these numbers, you can look at the decimal part. Since 0.11 is greater than 0.09 (or 0.9 - 0.9 = 0 and 0.11 - 0.09 = 0.02), 9.11 is greater than 9.9\n"
     ]
    }
   ],
   "source": [
    "response = llama31(prompt, 405)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b4c5e9-aeb2-48d7-bbae-2a36bbaba9c8",
   "metadata": {
    "id": "Eow6Kj9ruue_"
   },
   "source": [
    "Somehow the largest Llama 3.1 405b model returns the incorrect result. From the visualization of the tokens in the prompt, you can see the number 9.11 is split into 3 tokens: \"9\", \".\", and \".11\", while 9.9 into 2 tokens: \"9\", \".\", \"9\". If the two numbers are encoded as the two numbers themselves, correct model response will be more likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fff40b66-4c91-47c4-a575-fa4663ef1928",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 64,
    "id": "vUq9EC0-a8lv",
    "outputId": "b9bae0c5-cd84-43c7-8ca1-c99979d3afdb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(198, '\\n'),\n",
       " (128000, '<|begin_of_text|>'),\n",
       " (128006, '<|start_header_id|>'),\n",
       " (882, 'user'),\n",
       " (128007, '<|end_header_id|>'),\n",
       " (271, '\\n\\n'),\n",
       " (23956, 'Which'),\n",
       " (1396, ' number'),\n",
       " (374, ' is'),\n",
       " (11493, ' bigger'),\n",
       " (11, ','),\n",
       " (220, ' '),\n",
       " (24, '9'),\n",
       " (13, '.'),\n",
       " (806, '11'),\n",
       " (477, ' or'),\n",
       " (220, ' '),\n",
       " (24, '9'),\n",
       " (13, '.'),\n",
       " (24, '9'),\n",
       " (30, '?'),\n",
       " (220, ' '),\n",
       " (128009, '<|eot_id|>'),\n",
       " (128006, '<|start_header_id|>'),\n",
       " (78191, 'assistant'),\n",
       " (128007, '<|end_header_id|>'),\n",
       " (198, '\\n')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_tokens = tokenizer.encode(prompt, allowed_special=\"all\")\n",
    "decoded_tokens = [tokenizer.decode([token]) for token in encoded_tokens]\n",
    "[x for x in zip(encoded_tokens, decoded_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a7dfc789-327f-425d-9470-3cbe4b758d82",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "height": 30,
    "id": "Vk4FfGqme3Wx",
    "outputId": "1f7fc2f8-4921-4581-bbd8-909f7be142e4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"color: black; background-color: #ADE0FC; padding: 2px;\">\\n</span><span style=\"color: black; background-color: #FCE278; padding: 2px;\"><|begin_of_text|></span><span style=\"color: black; background-color: #B2D1FE; padding: 2px;\"><|start_header_id|></span><span style=\"color: black; background-color: #AFF7C6; padding: 2px;\">user</span><span style=\"color: black; background-color: #FDCE9B; padding: 2px;\"><|end_header_id|></span><span style=\"color: black; background-color: #97F1FB; padding: 2px;\">\\n\\n</span><span style=\"color: black; background-color: #DEE1E7; padding: 2px;\">Which</span><span style=\"color: black; background-color: #E3C9FF; padding: 2px;\"> number</span><span style=\"color: black; background-color: #BBC6FD; padding: 2px;\"> is</span><span style=\"color: black; background-color: #D1FB8C; padding: 2px;\"> bigger</span><span style=\"color: black; background-color: #ADE0FC; padding: 2px;\">,</span><span style=\"color: black; background-color: #FCE278; padding: 2px;\"> </span><span style=\"color: black; background-color: #B2D1FE; padding: 2px;\">9</span><span style=\"color: black; background-color: #AFF7C6; padding: 2px;\">.</span><span style=\"color: black; background-color: #FDCE9B; padding: 2px;\">11</span><span style=\"color: black; background-color: #97F1FB; padding: 2px;\"> or</span><span style=\"color: black; background-color: #DEE1E7; padding: 2px;\"> </span><span style=\"color: black; background-color: #E3C9FF; padding: 2px;\">9</span><span style=\"color: black; background-color: #BBC6FD; padding: 2px;\">.</span><span style=\"color: black; background-color: #D1FB8C; padding: 2px;\">9</span><span style=\"color: black; background-color: #ADE0FC; padding: 2px;\">?</span><span style=\"color: black; background-color: #FCE278; padding: 2px;\"> </span><span style=\"color: black; background-color: #B2D1FE; padding: 2px;\"><|eot_id|></span><span style=\"color: black; background-color: #AFF7C6; padding: 2px;\"><|start_header_id|></span><span style=\"color: black; background-color: #FDCE9B; padding: 2px;\">assistant</span><span style=\"color: black; background-color: #97F1FB; padding: 2px;\"><|end_header_id|></span><span style=\"color: black; background-color: #DEE1E7; padding: 2px;\">\\n</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(html_tokens(decoded_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da659067-b8dd-41bb-b0c2-28700e489c9e",
   "metadata": {
    "id": "8zF6KRbCssQ4"
   },
   "source": [
    "## String reversing\n",
    "\n",
    "First, for a common word \"amazing\", all 3 Llama 3.1 chat models reverse the string correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514f8d3d-0135-4914-8c5b-b0cfb9fb0420",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 149,
    "id": "z0NfW2ioylY2",
    "outputId": "d567cf95-d70c-4bbe-be5d-94e257b923b6"
   },
   "outputs": [],
   "source": [
    "input = \"Reverse the string 'amazing'\"\n",
    "prompt = f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "response = llama31(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ded4d6-4b8e-4a68-bed6-6e1dc4da831c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 47,
    "id": "h2TbIPdnylby",
    "outputId": "1a113ee7-789b-460e-e7d4-120e6e764cdc"
   },
   "outputs": [],
   "source": [
    "response = llama31(prompt, 70)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dced3e5-3812-49a4-a363-a977ca6653f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 47,
    "id": "05S3HqU7ylec",
    "outputId": "a46745bd-1e8c-402e-bb36-ba0bc810a8e0"
   },
   "outputs": [],
   "source": [
    "response = llama31(prompt, 405)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4f888c-58b4-4da9-87d9-94eb6b45ce1c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "height": 64,
    "id": "3JbyuLJG0erH",
    "outputId": "7d659c8e-2075-4b4b-f0bf-7289ade782b0"
   },
   "outputs": [],
   "source": [
    "encoded_tokens = tokenizer.encode(prompt, allowed_special=\"all\")\n",
    "decoded_tokens = [tokenizer.decode([token]) for token in encoded_tokens]\n",
    "display(HTML(html_tokens(decoded_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c640cc4-ac98-4bf4-85ea-0fd4acbd2561",
   "metadata": {
    "id": "d73S4v0DzZzR"
   },
   "source": [
    "For a less common word \"language\", Llama 3.1 8B doesn't return the correct result, but 70B and 405B do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69630cd-e495-4e07-bcab-3c899402dfb2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 149,
    "id": "o6zCMvEcx81n",
    "outputId": "0b9ef5da-89b6-4cf4-d86d-aaa3b5425551"
   },
   "outputs": [],
   "source": [
    "input = \"Reverse the string 'language'\"\n",
    "prompt = f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "response = llama31(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f643e37-800d-4a58-9d29-1e2180ebce11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 47,
    "id": "6wTtu2Dax84g",
    "outputId": "2ea3000f-78e9-424b-cb68-c9b6e0ad17cf"
   },
   "outputs": [],
   "source": [
    "response = llama31(prompt, 70)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b543a78e-2eb0-4ae3-ad78-df2626622855",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 47,
    "id": "xxNAXOWJx87s",
    "outputId": "e47fb971-1f3a-4fda-b49c-b00f1758b873"
   },
   "outputs": [],
   "source": [
    "response = llama31(prompt, 405)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5af398b5-dc0b-41c1-8c84-eecfdd8130ef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "height": 64,
    "id": "YjwtVk840xQZ",
    "outputId": "bd301af7-c0c5-4d90-fbe1-46ce44c53158"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"color: black; background-color: #ADE0FC; padding: 2px;\">\\n</span><span style=\"color: black; background-color: #FCE278; padding: 2px;\"><|begin_of_text|></span><span style=\"color: black; background-color: #B2D1FE; padding: 2px;\"><|start_header_id|></span><span style=\"color: black; background-color: #AFF7C6; padding: 2px;\">user</span><span style=\"color: black; background-color: #FDCE9B; padding: 2px;\"><|end_header_id|></span><span style=\"color: black; background-color: #97F1FB; padding: 2px;\">\\n\\n</span><span style=\"color: black; background-color: #DEE1E7; padding: 2px;\">Which</span><span style=\"color: black; background-color: #E3C9FF; padding: 2px;\"> number</span><span style=\"color: black; background-color: #BBC6FD; padding: 2px;\"> is</span><span style=\"color: black; background-color: #D1FB8C; padding: 2px;\"> bigger</span><span style=\"color: black; background-color: #ADE0FC; padding: 2px;\">,</span><span style=\"color: black; background-color: #FCE278; padding: 2px;\"> </span><span style=\"color: black; background-color: #B2D1FE; padding: 2px;\">9</span><span style=\"color: black; background-color: #AFF7C6; padding: 2px;\">.</span><span style=\"color: black; background-color: #FDCE9B; padding: 2px;\">11</span><span style=\"color: black; background-color: #97F1FB; padding: 2px;\"> or</span><span style=\"color: black; background-color: #DEE1E7; padding: 2px;\"> </span><span style=\"color: black; background-color: #E3C9FF; padding: 2px;\">9</span><span style=\"color: black; background-color: #BBC6FD; padding: 2px;\">.</span><span style=\"color: black; background-color: #D1FB8C; padding: 2px;\">9</span><span style=\"color: black; background-color: #ADE0FC; padding: 2px;\">?</span><span style=\"color: black; background-color: #FCE278; padding: 2px;\"> </span><span style=\"color: black; background-color: #B2D1FE; padding: 2px;\"><|eot_id|></span><span style=\"color: black; background-color: #AFF7C6; padding: 2px;\"><|start_header_id|></span><span style=\"color: black; background-color: #FDCE9B; padding: 2px;\">assistant</span><span style=\"color: black; background-color: #97F1FB; padding: 2px;\"><|end_header_id|></span><span style=\"color: black; background-color: #DEE1E7; padding: 2px;\">\\n</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_tokens = tokenizer.encode(prompt, allowed_special=\"all\")\n",
    "decoded_tokens = [tokenizer.decode([token]) for token in encoded_tokens]\n",
    "display(HTML(html_tokens(decoded_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f364dd4-a909-4f35-b813-a8e388e231d4",
   "metadata": {
    "id": "pQxwR6Dzz5kf"
   },
   "source": [
    "For the string \"XMLElement\", none of the 3 models is correct.\n",
    "### WHY???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "495880ee-9a77-4456-9f10-51b491604825",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 149,
    "id": "6gsPDJ8-s2nH",
    "outputId": "13436dc8-21a6-43eb-dc67-72d369cbc8ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reverse of 'XMLElement' is 'deMltxE'.\n"
     ]
    }
   ],
   "source": [
    "input = \"Reverse the string 'XMLElement'\"\n",
    "prompt = f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "response = llama31(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "11a30042-c245-4cb1-ac09-543b481384f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 47,
    "id": "HcvoNHTYtB5j",
    "outputId": "33783fa9-b0d2-4674-a92f-ae18591a48c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'tnemELX'\n"
     ]
    }
   ],
   "source": [
    "response = llama31(prompt, 70)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bf4a74a4-373e-4334-b9f6-51ba67724b69",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 64,
    "id": "rlwyKqSItB-G",
    "outputId": "26c856ca-eb85-4a00-9485-926fcd7db3f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reverse of 'XMLElement' is 'TNEMELEMX'.\n"
     ]
    }
   ],
   "source": [
    "response = llama31(prompt, 405)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "499a0e33-9072-4bc0-b478-0e84dc773744",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "height": 64,
    "id": "K8EXFGUt05Dw",
    "outputId": "cf8c3a9e-0eef-4de5-85c4-05bb86f5c7d8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"color: black; background-color: #ADE0FC; padding: 2px;\">\\n</span><span style=\"color: black; background-color: #FCE278; padding: 2px;\"><|begin_of_text|></span><span style=\"color: black; background-color: #B2D1FE; padding: 2px;\"><|start_header_id|></span><span style=\"color: black; background-color: #AFF7C6; padding: 2px;\">user</span><span style=\"color: black; background-color: #FDCE9B; padding: 2px;\"><|end_header_id|></span><span style=\"color: black; background-color: #97F1FB; padding: 2px;\">\\n\\n</span><span style=\"color: black; background-color: #DEE1E7; padding: 2px;\">Reverse</span><span style=\"color: black; background-color: #E3C9FF; padding: 2px;\"> the</span><span style=\"color: black; background-color: #BBC6FD; padding: 2px;\"> string</span><span style=\"color: black; background-color: #D1FB8C; padding: 2px;\"> '</span><span style=\"color: black; background-color: #ADE0FC; padding: 2px;\">XMLElement</span><span style=\"color: black; background-color: #FCE278; padding: 2px;\">'</span><span style=\"color: black; background-color: #B2D1FE; padding: 2px;\"><|eot_id|></span><span style=\"color: black; background-color: #AFF7C6; padding: 2px;\"><|start_header_id|></span><span style=\"color: black; background-color: #FDCE9B; padding: 2px;\">assistant</span><span style=\"color: black; background-color: #97F1FB; padding: 2px;\"><|end_header_id|></span><span style=\"color: black; background-color: #DEE1E7; padding: 2px;\">\\n</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_tokens = tokenizer.encode(prompt, allowed_special=\"all\")\n",
    "decoded_tokens = [tokenizer.decode([token]) for token in encoded_tokens]\n",
    "display(HTML(html_tokens(decoded_tokens)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
